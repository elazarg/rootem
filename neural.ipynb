{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sn\n",
    "\n",
    "import utils\n",
    "from naive_model import NaiveModel\n",
    "import encoding\n",
    "\n",
    "NUM_EMBEDDING = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "def to_device(d):\n",
    "    if hasattr(d, 'cuda'):\n",
    "        return d.cuda()\n",
    "    return {k: v.cuda() for k, v in d.items()}\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, units, combinations=encoding.NAMES):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        self.lstm = nn.LSTM(input_size=units, hidden_size=units, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.tasks = {}\n",
    "        for tup in combinations:\n",
    "            shape = tuple(len(encoding.CLASSES[k]) for k in tup)\n",
    "            out = nn.Linear(in_features=units, out_features=np.prod(shape))\n",
    "            self.tasks[tup] = (out, shape)\n",
    "            setattr(self, '+'.join(tup), out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
    "        left, right = torch.chunk(h_n, 2, dim=0)\n",
    "        merge = torch.squeeze(left + right)\n",
    "\n",
    "        outputs = { k: f(merge).view(shape) for k, (f, shape) in self.tasks.items() }\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 'T')\n",
      "[[-0.03073758  0.12819305 -0.07848827 -0.00512525]\n",
      " [ 0.03981335  0.08570858  0.04568484 -0.17644502]\n",
      " [ 0.22094595  0.15571463 -0.25609243  0.03385381]\n",
      " [-0.01417239  0.29071298  0.09653775 -0.07986734]\n",
      " [ 0.10159156  0.03317711 -0.06815157 -0.05779996]\n",
      " [ 0.11075711  0.02361325 -0.13230067  0.15099898]\n",
      " [-0.05639153  0.03840695  0.01949406 -0.05879   ]]\n",
      "\n",
      "('T', 'G')\n",
      "[[ 0.02805089 -0.10407012]\n",
      " [ 0.04608866  0.02199431]\n",
      " [-0.19873065 -0.13313344]\n",
      " [-0.05153071  0.13049652]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sanity():\n",
    "    model = Model(100, combinations=[('B', 'T'), ('T', 'G')])\n",
    "    with torch.no_grad():\n",
    "        verbs = encoding.wordlist2numpy([\"כשאתאקלם\"])\n",
    "        verbs = torch.from_numpy(verbs).to(torch.int64)\n",
    "        tag_scores = model(verbs)\n",
    "        for k in tag_scores:\n",
    "            print(k)\n",
    "            v = tag_scores[k].detach().numpy()\n",
    "            print(v)\n",
    "            print()\n",
    "\n",
    "sanity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concrete\n",
    "\n",
    "def load_dataset(file_pat):\n",
    "    *features_train, verbs_train = concrete.load_dataset(f'{file_pat}_train.tsv')\n",
    "    *features_test, verbs_test = concrete.load_dataset(f'{file_pat}_test.tsv')\n",
    "    return ((encoding.wordlist2numpy(verbs_train), encoding.list_of_lists_to_category(features_train)),\n",
    "            (encoding.wordlist2numpy(verbs_test), encoding.list_of_lists_to_category(features_test)))\n",
    "\n",
    "def load_dataset_split(filename, split):\n",
    "    *features_train, verbs_train = concrete.load_dataset(filename)\n",
    "    features_test = [t[-split:] for t in features_train]\n",
    "    verbs_test = verbs_train[-split:]\n",
    "    del verbs_train[-split:]\n",
    "    for t in features_train:\n",
    "        del t[-split:]\n",
    "    return ((wordlist2numpy(verbs_train), list_of_lists_to_category(features_train)),\n",
    "            (wordlist2numpy(verbs_test ), list_of_lists_to_category(features_test )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def batch(a):\n",
    "    ub = a.shape[0] // BATCH_SIZE * BATCH_SIZE\n",
    "    return to_device(torch.from_numpy(a[:ub]).to(torch.int64)).split(BATCH_SIZE)\n",
    "\n",
    "def batch_all_ys(ys):\n",
    "    res = []\n",
    "    m = {k: batch(ys[k]) for k in encoding.NAMES}\n",
    "    nbatches = len(m['B'])\n",
    "    for i in range(nbatches):\n",
    "        res.append({k: m[k][i] for k in encoding.NAMES})\n",
    "    return res\n",
    "\n",
    "def fit(model, train, test, *, epochs,  runsize, criterion, optimizer, phases, teacher):\n",
    "    x_train, y_train = train\n",
    "    x_test, y_test = test\n",
    "    data = {\n",
    "        'train': (batch(x_train), batch_all_ys(y_train)),\n",
    "        'test':  (batch(x_test ), batch_all_ys(y_test ))\n",
    "    }\n",
    "\n",
    "    stats = utils.Stats(model.tasks.keys(), runsize)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        stats.epoch_start()\n",
    "        \n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            stats.phase_start(phase, batches_in_phase=len(data[phase][0]))\n",
    "\n",
    "            for inputs, labels in zip(*data[phase]):\n",
    "                stats.batch_start()\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    outputs = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                if teacher is not None:\n",
    "                    pseudo_labels = teacher(inputs)\n",
    "                    losses = {k: criterion(outputs[k].double(), pseudo_labels[k]) for k in outputs}\n",
    "                else:\n",
    "                    losses = {k: criterion(outputs[k].double(), labels[k]) for k in outputs}\n",
    "                \n",
    "                if phase == 'train' and isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                    stats.assert_resonable_initial(losses)\n",
    "                \n",
    "                loss = sum(losses.values())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                stats.update(loss=loss.item(),\n",
    "                             batch_size=inputs.size(0),\n",
    "                             d={k: (outputs[k], labels[k].detach()) for k in outputs})\n",
    "                \n",
    "                stats.batch_end()\n",
    "            stats.phase_end()\n",
    "        stats.epoch_end()\n",
    "    return stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, *verbs):\n",
    "    model.eval()\n",
    "    verbs = encoding.wordlist2numpy(verbs)\n",
    "    verbs = to_device(torch.from_numpy(verbs).to(torch.int64))\n",
    "    outputs = model(verbs)\n",
    "    res = {k: encoding.from_category(k, torch.argmax(v))\n",
    "           for k, v in outputs.items()}\n",
    "    res['R'] = ''.join(res[k] for k in ['R1', 'R2', 'R3', 'R4']).replace('.', '')\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arity = '3'\n",
    "gen = 'all'\n",
    "artifact_name = f'{gen}_{arity}_shuffled'\n",
    "filename = f'synthetic/{artifact_name}.tsv'  # all_verbs_shuffled\n",
    "test_size = 5000\n",
    "\n",
    "artifact = wandb.Artifact(artifact_name, type='dataset')\n",
    "artifact.add_file(filename)\n",
    "\n",
    "train, test = load_dataset_split(filename, split=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_config(filename):\n",
    "    return {\n",
    "        'model': NaiveModel.learn_from_file(filename),\n",
    "        'phases': ['test'],\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'optimizer': None\n",
    "    }\n",
    "\n",
    "def teacher_config(train):\n",
    "    res = standard_config()\n",
    "    res['teacher'] = NaiveModel.learn_from_data(train)\n",
    "    res['criterion'] = nn.BCEWithLogitsLoss()  # BCELoss: works, but total loss is nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def standard_config(model, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return {\n",
    "        'model': model,\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'optimizer': optimizer,\n",
    "        'phases': ['train', 'test'],\n",
    "        'teacher': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env WANDB_SILENT true\n",
    "%env WANDB_MODE run\n",
    "# os.environ['WANDB_MODE'] = 'run'  # 'dryrun'\n",
    "\n",
    "config = {\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': 1,\n",
    "    'runsize': 2 * 8192 // BATCH_SIZE,\n",
    "    'test_size': test_size,\n",
    "}\n",
    "# group = \n",
    "\n",
    "def experiment(names, root=True):\n",
    "    units = 400\n",
    "    lr = 8e-4\n",
    "\n",
    "    config.update({\n",
    "        'units': units,\n",
    "        'lr': lr,\n",
    "    })\n",
    "    names_str = '+'.join(names)\n",
    "    if root:\n",
    "        names += ['R1', 'R2', 'R3', 'R4']\n",
    "        isroot = ''\n",
    "    else:\n",
    "        isroot = 'no'\n",
    "    run = wandb.init(project=\"rootem\",\n",
    "                     group=f'subsets-{gen}-{arity}-{isroot}root',  # f'lr_units_grid_search-{arity}-{wandb.util.generate_id()}',\n",
    "                     name=f'{gen}-{arity}-{names_str}-{isroot}root',  # f'{gen}-{arity}-{lr:.0e}',# f'{arity}-batch_{BATCH_SIZE}', # f'all-{arity}-lr_{lr:.0e}-units_{units}',\n",
    "                     tags=[gen, arity, 'synthetic', 'shuffle', 'no_prefix'],\n",
    "                     config=config)\n",
    "\n",
    "    run.use_artifact(artifact)\n",
    "\n",
    "    wandb.config.update(config, allow_val_change=True)\n",
    "\n",
    "    print(config)\n",
    "    \n",
    "    model = to_device(Model(units=units, names=names))\n",
    "    if isinstance(model, nn.Module):\n",
    "        wandb.watch(model)\n",
    "        \n",
    "    stats = fit(train=train,\n",
    "        test=test,\n",
    "        epochs=config['epochs'],\n",
    "        runsize=config['runsize'],\n",
    "        **standard_config(model, lr)\n",
    "    )\n",
    "    # wandb.save(f\"simple_{arity}.h5\")\n",
    "    return stats\n",
    "\n",
    "# lr = 8e-4, 10e-4, 20e-4, 30e-4, 40e-4, 50e-4, 60e-4]:\n",
    "def nonempty_powerset(seq):\n",
    "    return itertools.chain.from_iterable(itertools.combinations(seq, r) for r in range(1, len(seq)+1))\n",
    "\n",
    "# for root in [False, True]:\n",
    "#     for subnames in nonempty_powerset(encoding.NAMES[:-4]):\n",
    "#         print(subnames)\n",
    "stats = experiment(names=[], root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'R1'\n",
    "labels = [x[::-1] for x in CLASSES[k]]\n",
    "ax = sn.heatmap(stats.confusion[k], xticklabels=labels, yticklabels=labels, square=True, robust=True, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sn.heatmap(stats.confusion_logprobs[k], xticklabels=labels, yticklabels=labels, square=True, robust=True, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, 'סבסו'))\n",
    "print(predict(model, 'מקדו'))\n",
    "print(predict(model, 'נמזר'))\n",
    "print(predict(model, 'כרדו'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, 'הבריל'))\n",
    "print(predict(model, 'חגוו'))\n",
    "print(predict(model, 'עגו'))\n",
    "print(predict(model, 'צירלל'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, \"השטקרפתי\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, \"ישסו\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import encoding\n",
    "import naive_model\n",
    "import utils\n",
    "encoding = importlib.reload(encoding)\n",
    "naive_model = importlib.reload(naive_model)\n",
    "utils = importlib.reload(utils)\n",
    "wandb = importlib.reload(wandb)\n",
    "Stats = utils.Stats\n",
    "NaiveModel = naive_model.NaiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "labels = [x[::-1] for x in CLASSES['B']]\n",
    "# wandb.plots.HeatMap(labels, labels, stats.confusion['B'])\n",
    "help(wandb.plots.heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nonempty_powerset(NAMES[:-4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_classes(a, b):\n",
    "    return ['.'.join(x) for x in itertools.product(a, b)]\n",
    "\n",
    "combine_classes(CLASSES['B'], CLASSES['V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
