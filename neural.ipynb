{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import more_itertools as mi\n",
    "\n",
    "NUM_EMBEDDING = 2000\n",
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from naive_model import NaiveModel\n",
    "import encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "assert torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def to_device(d):\n",
    "    if hasattr(d, 'cuda'):\n",
    "        return d.cuda()\n",
    "    return {k: v.cuda() for k, v in d.items()}\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, units, combinations=encoding.NAMES):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        self.lstm = nn.LSTM(input_size=units, hidden_size=units, num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.tasks = {}\n",
    "        for combination in combinations:\n",
    "            out = nn.Linear(in_features=units, out_features=encoding.class_size(combination))\n",
    "            self.tasks[combination] = out\n",
    "            setattr(self, encoding.class_name(combination), out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)\n",
    "\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds)\n",
    "        left, right = torch.chunk(h_n, 2, dim=0)\n",
    "        merge = torch.squeeze(left + right)\n",
    "\n",
    "        outputs = { combination: f(merge) for combination, f in self.tasks.items() }\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator as op\n",
    "\n",
    "def sanity():\n",
    "    model = to_device(Model(100, combinations=[('B', 'T')]))\n",
    "    print(model)\n",
    "    with torch.no_grad():\n",
    "        verbs = encoding.wordlist2numpy([\"אתאקלם\", \"יכפיל\"])\n",
    "        labels = {'B': torch.Tensor([3, 5]), 'T': torch.Tensor([2, 4])}\n",
    "        verbs = to_device(torch.from_numpy(verbs).to(torch.int64))\n",
    "        tag_scores = model(verbs)\n",
    "        for combination in tag_scores:\n",
    "            print(combination)\n",
    "            v = tag_scores[combination]\n",
    "            c_labels = functools.reduce(op.mul, [labels[k] for k in combination])\n",
    "            print(f'{v=}')\n",
    "            print(f'{c_labels=}')\n",
    "            print(f'{labels=}')\n",
    "            print()\n",
    "\n",
    "sanity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concrete\n",
    "\n",
    "def load_dataset(file_pat):\n",
    "    *features_train, verbs_train = concrete.load_dataset(f'{file_pat}_train.tsv')\n",
    "    *features_test, verbs_test = concrete.load_dataset(f'{file_pat}_test.tsv')\n",
    "    return ((encoding.wordlist2numpy(verbs_train), encoding.list_of_lists_to_category(features_train)),\n",
    "            (encoding.wordlist2numpy(verbs_test) , encoding.list_of_lists_to_category(features_test )))\n",
    "\n",
    "def load_dataset_split(filename, split):\n",
    "    *features_train, verbs_train = concrete.load_dataset(filename)\n",
    "    features_test = [t[-split:] for t in features_train]\n",
    "    verbs_test = verbs_train[-split:]\n",
    "    del verbs_train[-split:]\n",
    "    for t in features_train:\n",
    "        del t[-split:]\n",
    "    return ((encoding.wordlist2numpy(verbs_train), encoding.list_of_lists_to_category(features_train)),\n",
    "            (encoding.wordlist2numpy(verbs_test ), encoding.list_of_lists_to_category(features_test )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def batch(a):\n",
    "    ub = a.shape[0] // BATCH_SIZE * BATCH_SIZE\n",
    "    return to_device(torch.from_numpy(a[:ub]).to(torch.int64)).split(BATCH_SIZE)\n",
    "\n",
    "def ravel_multi_index(ys, combinations):\n",
    "    nsamples = len(next(iter(ys.values())))\n",
    "    return {combination: ys[combination] if not isinstance(combination, (tuple, list))\n",
    "                   else (ys[combination[0]] if len(combination) == 0\n",
    "                   else np.ravel_multi_index([ys[k] for k in combination], encoding.combined_shape(combination)))\n",
    "            for combination in combinations}\n",
    "\n",
    "def batch_all_ys(ys):\n",
    "    res = []\n",
    "    m = {k: batch(ys[k]) for k in ys}\n",
    "    nbatches = len(next(iter(m.values())))\n",
    "    for i in range(nbatches):\n",
    "        res.append({k: m[k][i] for k in ys})\n",
    "    return res\n",
    "\n",
    "def fit(model, train, test, *, epochs,  runsize, criterion, optimizer, phases, teacher):\n",
    "    x_train, y_train = train\n",
    "    x_test, y_test = test\n",
    "    data = {\n",
    "        'train': (batch(x_train), batch_all_ys(y_train)),\n",
    "        'test':  (batch(x_test ), batch_all_ys(y_test ))\n",
    "    }\n",
    "\n",
    "    stats = utils.Stats(model.tasks.keys(), runsize)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        stats.epoch_start()\n",
    "        \n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            stats.phase_start(phase, batches_in_phase=len(data[phase][0]))\n",
    "\n",
    "            for inputs, labels in zip(*data[phase]):\n",
    "                inputs = to_device(inputs)\n",
    "                labels = to_device(labels)\n",
    "                stats.batch_start()\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    outputs = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                if teacher is not None:\n",
    "                    pseudo_labels = teacher(inputs)\n",
    "                    losses = {k: criterion(outputs[k].double(), pseudo_labels[k]) for k in outputs}\n",
    "                else:\n",
    "                    losses = {combination: criterion(output.double(), labels[combination])\n",
    "                              for combination, output in outputs.items()}\n",
    "                \n",
    "                # if phase == 'train' and isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                #     stats.assert_reasonable_initial(losses)\n",
    "                \n",
    "                loss = sum(losses.values())\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                stats.update(loss=loss.item(),\n",
    "                             batch_size=inputs.size(0),\n",
    "                             d={k: (outputs[k], labels[k]) for k in outputs})\n",
    "                \n",
    "                stats.batch_end()\n",
    "            stats.phase_end()\n",
    "        stats.epoch_end()\n",
    "    return stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, *verbs):\n",
    "    model.eval()\n",
    "    verbs = encoding.wordlist2numpy(verbs)\n",
    "    verbs = to_device(torch.from_numpy(verbs).to(torch.int64))\n",
    "    outputs = model(verbs)\n",
    "    res = {}\n",
    "    # FIX: assumes no overlaps\n",
    "    for combination, v in outputs.items():\n",
    "        combined_index = torch.argmax(v).cpu().data.numpy()\n",
    "        indices = np.unravel_index(combined_index, encoding.combined_shape(combination))\n",
    "        for k, i in zip(combination, indices):\n",
    "            # assert k not in res, \"Overlapping classes are not handled\"\n",
    "            s = k\n",
    "            if k in res:\n",
    "                s += \"'\"\n",
    "            res[s] = encoding.from_category(k, i)\n",
    "    if all(r in res for r in ['R1', 'R2', 'R3', 'R4']):\n",
    "        res['R'] = ''.join(res[k] for k in ['R1', 'R2', 'R3', 'R4']).replace('.', '')\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "arity = '3'\n",
    "gen = 'all'\n",
    "artifact_name = f'{gen}_{arity}_shufroot'\n",
    "filename = f'synthetic/{artifact_name}.tsv'  # all_verbs_shuffled\n",
    "test_size = 5000\n",
    "\n",
    "artifact = wandb.Artifact(artifact_name, type='dataset')\n",
    "artifact.add_file(filename)\n",
    "\n",
    "(train_x, pre_train_y), (test_x, pre_test_y) = load_dataset_split(filename, split=test_size)\n",
    "\n",
    "def shuffle_in_unison_scary(arrs):\n",
    "    rng_state = np.random.get_state()\n",
    "    for arr in arrs:\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(arr)\n",
    "shuffle_in_unison_scary([train_x, *pre_train_y.values()])\n",
    "# TODO: shuffle train_x and pre_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_config(filename):\n",
    "    return {\n",
    "        'model': NaiveModel.learn_from_file(filename),\n",
    "        'phases': ['test'],\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'optimizer': None\n",
    "    }\n",
    "\n",
    "def teacher_config(train):\n",
    "    res = standard_config()\n",
    "    res['teacher'] = NaiveModel.learn_from_data(train)\n",
    "    res['criterion'] = nn.BCEWithLogitsLoss()  # BCELoss: works, but total loss is nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def standard_config(model, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return {\n",
    "        'model': model,\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'optimizer': optimizer,\n",
    "        'phases': ['train', 'test'],\n",
    "        'teacher': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: WANDB_MODE=run\n",
      "28/52: [('P',), ('B', 'V', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/299bcg86\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/299bcg86</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.980 BxVxG_acc: 0.440 T_acc: 0.931 B_acc: 0.820 V_acc: 0.685 G_acc: 0.788 Loss: 1.3253\n",
      " 1    39/   39 P_acc: 0.979 BxVxG_acc: 0.447 T_acc: 0.919 B_acc: 0.811 V_acc: 0.689 G_acc: 0.793 Loss: 1.3804\n",
      "29/52: [('P',), ('V', 'G'), ('B', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2dv5o7al\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2dv5o7al</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.980 VxG_acc: 0.533 BxT_acc: 0.791 V_acc: 0.690 G_acc: 0.787 B_acc: 0.826 T_acc: 0.929 Loss: 1.2992\n",
      " 1    39/   39 P_acc: 0.979 VxG_acc: 0.535 BxT_acc: 0.768 V_acc: 0.690 G_acc: 0.790 B_acc: 0.803 T_acc: 0.913 Loss: 1.3684\n",
      "30/52: [('B', 'P'), ('G',), ('V', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2u6qr6pd\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2u6qr6pd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 BxP_acc: 0.820 G_acc: 0.795 VxT_acc: 0.679 B_acc: 0.827 P_acc: 0.979 V_acc: 0.694 T_acc: 0.925 Loss: 1.2936\n",
      " 1    39/   39 BxP_acc: 0.806 G_acc: 0.801 VxT_acc: 0.672 B_acc: 0.814 P_acc: 0.977 V_acc: 0.692 T_acc: 0.912 Loss: 1.3573\n",
      "31/52: [('P',), ('B', 'G'), ('V', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2lcpvmmu\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2lcpvmmu</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.981 BxG_acc: 0.665 VxT_acc: 0.680 B_acc: 0.827 G_acc: 0.793 V_acc: 0.694 T_acc: 0.925 Loss: 1.3001\n",
      " 1    39/   39 P_acc: 0.978 BxG_acc: 0.652 VxT_acc: 0.668 B_acc: 0.810 G_acc: 0.794 V_acc: 0.689 T_acc: 0.910 Loss: 1.3850\n",
      "32/52: [('P',), ('G',), ('B', 'V', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/3j2xkw7k\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/3j2xkw7k</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.980 G_acc: 0.797 BxVxT_acc: 0.568 B_acc: 0.819 V_acc: 0.693 T_acc: 0.925 Loss: 1.2466\n",
      " 1    39/   39 P_acc: 0.979 G_acc: 0.799 BxVxT_acc: 0.562 B_acc: 0.797 V_acc: 0.694 T_acc: 0.916 Loss: 1.3121\n",
      "33/52: [('B', 'P', 'V'), ('G',), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/3rtl3itl\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/3rtl3itl</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 BxPxV_acc: 0.572 G_acc: 0.794 T_acc: 0.931 B_acc: 0.821 P_acc: 0.979 V_acc: 0.693 Loss: 1.3610\n",
      " 1    39/   39 BxPxV_acc: 0.575 G_acc: 0.801 T_acc: 0.917 B_acc: 0.809 P_acc: 0.978 V_acc: 0.689 Loss: 1.4103\n",
      "34/52: [('P', 'V'), ('B', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/3i8i07my\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/3i8i07my</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 PxV_acc: 0.695 BxG_acc: 0.662 T_acc: 0.932 P_acc: 0.981 V_acc: 0.699 B_acc: 0.825 G_acc: 0.792 Loss: 1.3893\n",
      " 1    39/   39 PxV_acc: 0.689 BxG_acc: 0.657 T_acc: 0.915 P_acc: 0.979 V_acc: 0.694 B_acc: 0.814 G_acc: 0.796 Loss: 1.4477\n",
      "35/52: [('P', 'V'), ('G',), ('B', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2zz6qfk3\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2zz6qfk3</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 PxV_acc: 0.693 G_acc: 0.793 BxT_acc: 0.791 P_acc: 0.981 V_acc: 0.698 B_acc: 0.825 T_acc: 0.927 Loss: 1.3295\n",
      " 1    39/   39 PxV_acc: 0.688 G_acc: 0.798 BxT_acc: 0.774 P_acc: 0.979 V_acc: 0.692 B_acc: 0.809 T_acc: 0.913 Loss: 1.3898\n",
      "36/52: [('B', 'V'), ('P', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/12vmf0x0\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/12vmf0x0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 BxV_acc: 0.575 PxG_acc: 0.781 T_acc: 0.931 B_acc: 0.823 V_acc: 0.692 P_acc: 0.979 G_acc: 0.791 Loss: 1.3707\n",
      " 1    39/   39 BxV_acc: 0.577 PxG_acc: 0.786 T_acc: 0.914 B_acc: 0.810 V_acc: 0.693 P_acc: 0.979 G_acc: 0.795 Loss: 1.4418\n",
      "37/52: [('V',), ('B', 'P', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/f41o9epi\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/f41o9epi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 V_acc: 0.697 BxPxG_acc: 0.660 T_acc: 0.932 B_acc: 0.826 P_acc: 0.977 G_acc: 0.791 Loss: 1.3880\n",
      " 1    39/   39 V_acc: 0.694 BxPxG_acc: 0.657 T_acc: 0.918 B_acc: 0.819 P_acc: 0.977 G_acc: 0.796 Loss: 1.4251\n",
      "38/52: [('V',), ('P', 'G'), ('B', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/10xii7l4\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/10xii7l4</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 V_acc: 0.696 PxG_acc: 0.782 BxT_acc: 0.790 P_acc: 0.980 G_acc: 0.792 B_acc: 0.825 T_acc: 0.928 Loss: 1.3439\n",
      " 1    39/   39 V_acc: 0.694 PxG_acc: 0.785 BxT_acc: 0.771 P_acc: 0.975 G_acc: 0.797 B_acc: 0.806 T_acc: 0.912 Loss: 1.3976\n",
      "39/52: [('B', 'V'), ('G',), ('P', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/1hznr1vj\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/1hznr1vj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 BxV_acc: 0.574 G_acc: 0.797 PxT_acc: 0.928 B_acc: 0.820 V_acc: 0.694 P_acc: 0.981 T_acc: 0.930 Loss: 1.3462\n",
      " 1    39/   39 BxV_acc: 0.572 G_acc: 0.799 PxT_acc: 0.917 B_acc: 0.803 V_acc: 0.688 P_acc: 0.980 T_acc: 0.917 Loss: 1.4165\n",
      "40/52: [('V',), ('B', 'G'), ('P', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2eujay1i\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2eujay1i</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 V_acc: 0.697 BxG_acc: 0.665 PxT_acc: 0.930 B_acc: 0.825 G_acc: 0.795 P_acc: 0.981 T_acc: 0.931 Loss: 1.3738\n",
      " 1    39/   39 V_acc: 0.692 BxG_acc: 0.655 PxT_acc: 0.916 B_acc: 0.813 G_acc: 0.795 P_acc: 0.980 T_acc: 0.916 Loss: 1.4373\n",
      "41/52: [('V',), ('G',), ('B', 'P', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/1aw8f88b\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/1aw8f88b</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 V_acc: 0.696 G_acc: 0.795 BxPxT_acc: 0.787 B_acc: 0.822 P_acc: 0.980 T_acc: 0.928 Loss: 1.3253\n",
      " 1    39/   39 V_acc: 0.695 G_acc: 0.800 BxPxT_acc: 0.780 B_acc: 0.814 P_acc: 0.977 T_acc: 0.915 Loss: 1.3644\n",
      "42/52: [('B',), ('P',), ('V',), ('G', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/143io7qe\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/143io7qe</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.827 P_acc: 0.980 V_acc: 0.696 GxT_acc: 0.739 G_acc: 0.785 T_acc: 0.928 Loss: 1.3842\n",
      " 1    39/   39 B_acc: 0.812 P_acc: 0.981 V_acc: 0.694 GxT_acc: 0.740 G_acc: 0.794 T_acc: 0.914 Loss: 1.4569\n",
      "43/52: [('B',), ('P',), ('V', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2l5mgvsp\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2l5mgvsp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.827 P_acc: 0.980 VxG_acc: 0.536 T_acc: 0.930 V_acc: 0.692 G_acc: 0.791 Loss: 1.3618\n",
      " 1    39/   39 B_acc: 0.810 P_acc: 0.976 VxG_acc: 0.537 T_acc: 0.914 V_acc: 0.690 G_acc: 0.791 Loss: 1.4644\n",
      "44/52: [('B',), ('P',), ('G',), ('V', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/240hux51\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/240hux51</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.828 P_acc: 0.981 G_acc: 0.796 VxT_acc: 0.680 V_acc: 0.695 T_acc: 0.927 Loss: 1.3031\n",
      " 1    39/   39 B_acc: 0.812 P_acc: 0.977 G_acc: 0.798 VxT_acc: 0.673 V_acc: 0.693 T_acc: 0.913 Loss: 1.3965\n",
      "45/52: [('B',), ('P', 'V'), ('G',), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/1fdua8f0\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/1fdua8f0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.827 PxV_acc: 0.695 G_acc: 0.797 T_acc: 0.930 P_acc: 0.981 V_acc: 0.699 Loss: 1.3964\n",
      " 1    39/   39 B_acc: 0.810 PxV_acc: 0.687 G_acc: 0.799 T_acc: 0.919 P_acc: 0.978 V_acc: 0.693 Loss: 1.4792\n",
      "46/52: [('B',), ('V',), ('P', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/43kx8ns0\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/43kx8ns0</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.827 V_acc: 0.696 PxG_acc: 0.784 T_acc: 0.932 P_acc: 0.980 G_acc: 0.794 Loss: 1.4019\n",
      " 1    39/   39 B_acc: 0.819 V_acc: 0.693 PxG_acc: 0.787 T_acc: 0.917 P_acc: 0.977 G_acc: 0.799 Loss: 1.4791\n",
      "47/52: [('B',), ('V',), ('G',), ('P', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/vtqm1dd6\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/vtqm1dd6</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.826 V_acc: 0.696 G_acc: 0.794 PxT_acc: 0.930 P_acc: 0.981 T_acc: 0.932 Loss: 1.3771\n",
      " 1    39/   39 B_acc: 0.808 V_acc: 0.697 G_acc: 0.799 PxT_acc: 0.914 P_acc: 0.979 T_acc: 0.914 Loss: 1.4610\n",
      "48/52: [('B', 'P'), ('V',), ('G',), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2s244r0f\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2s244r0f</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 BxP_acc: 0.819 V_acc: 0.696 G_acc: 0.797 T_acc: 0.931 B_acc: 0.826 P_acc: 0.979 Loss: 1.3955\n",
      " 1    39/   39 BxP_acc: 0.806 V_acc: 0.696 G_acc: 0.799 T_acc: 0.914 B_acc: 0.814 P_acc: 0.976 Loss: 1.4720\n",
      "49/52: [('P',), ('B', 'V'), ('G',), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/11rmupfq\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/11rmupfq</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.981 BxV_acc: 0.572 G_acc: 0.793 T_acc: 0.932 B_acc: 0.820 V_acc: 0.693 Loss: 1.3747\n",
      " 1    39/   39 P_acc: 0.978 BxV_acc: 0.579 G_acc: 0.801 T_acc: 0.914 B_acc: 0.810 V_acc: 0.695 Loss: 1.4440\n",
      "50/52: [('P',), ('V',), ('B', 'G'), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/2r514na2\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/2r514na2</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.981 V_acc: 0.697 BxG_acc: 0.663 T_acc: 0.932 B_acc: 0.826 G_acc: 0.791 Loss: 1.4000\n",
      " 1    39/   39 P_acc: 0.979 V_acc: 0.694 BxG_acc: 0.657 T_acc: 0.916 B_acc: 0.814 G_acc: 0.798 Loss: 1.4743\n",
      "51/52: [('P',), ('V',), ('G',), ('B', 'T')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/13qmlhag\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/13qmlhag</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 P_acc: 0.980 V_acc: 0.699 G_acc: 0.796 BxT_acc: 0.788 B_acc: 0.822 T_acc: 0.928 Loss: 1.3486\n",
      " 1    39/   39 P_acc: 0.978 V_acc: 0.692 G_acc: 0.800 BxT_acc: 0.779 B_acc: 0.814 T_acc: 0.913 Loss: 1.3927\n",
      "52/52: [('B',), ('P',), ('V',), ('G',), ('T',)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/rootem\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/rootem/runs/1nikvvqb\" target=\"_blank\">https://app.wandb.ai/elazarg/rootem/runs/1nikvvqb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'batch_size': 128, 'epochs': 1, 'runsize': 128, 'test_size': 5000, 'units': 400, 'lr': 0.0008}\n",
      " 1  5376/ 5470 B_acc: 0.827 P_acc: 0.979 V_acc: 0.696 G_acc: 0.793 T_acc: 0.930 Loss: 1.4110\n",
      " 1    39/   39 B_acc: 0.808 P_acc: 0.978 V_acc: 0.694 G_acc: 0.800 T_acc: 0.914 Loss: 1.4965\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e66649fb51ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment_partitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;31m# TODO: statistics for each k in each combination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT true\n",
    "%env WANDB_MODE run\n",
    "\n",
    "config = {\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': 1,\n",
    "    'runsize': 2 * 8192 // BATCH_SIZE,\n",
    "    'test_size': test_size,\n",
    "}\n",
    "\n",
    "def experiment(combinations):\n",
    "    train_y = ravel_multi_index(pre_train_y, combinations)\n",
    "    test_y = ravel_multi_index(pre_test_y, combinations)\n",
    "    train = (train_x, train_y)\n",
    "    test = (test_x, test_y)\n",
    "    \n",
    "    units = 400\n",
    "    lr = 8e-4\n",
    "\n",
    "    config.update({\n",
    "        'units': units,\n",
    "        'lr': lr,\n",
    "    })\n",
    "    names_str = '+'.join(encoding.class_name(combination) for combination in combinations)\n",
    "    run = wandb.init(project=\"rootem\",\n",
    "                     group=f'joint-{gen}-{arity}-noroot',  # f'lr_units_grid_search-{arity}-{wandb.util.generate_id()}',\n",
    "                     name=f'joint-{gen}-{arity}-{names_str}-noroot',  # f'{gen}-{arity}-{lr:.0e}',# f'{arity}-batch_{BATCH_SIZE}', # f'all-{arity}-lr_{lr:.0e}-units_{units}',\n",
    "                     tags=[gen, arity, 'synthetic', 'shuffle-root', 'no_prefix', 'shuffle', 'partitions'],\n",
    "                     config=config)\n",
    "    with run:\n",
    "        run.use_artifact(artifact)\n",
    "\n",
    "        wandb.config.update(config, allow_val_change=True)\n",
    "\n",
    "        print(config)\n",
    "\n",
    "        model = to_device(Model(units=units, combinations=combinations))\n",
    "        if isinstance(model, nn.Module):\n",
    "            wandb.watch(model)\n",
    "\n",
    "        stats = fit(train=train,\n",
    "            test=test,\n",
    "            epochs=config['epochs'],\n",
    "            runsize=config['runsize'],\n",
    "            **standard_config(model, lr)\n",
    "        )\n",
    "        wandb.save(f\"simple_{arity}.h5\")\n",
    "    # wandb.join()\n",
    "    return model, stats\n",
    "\n",
    "def experiment_partitions():\n",
    "    import more_itertools as mi\n",
    "    partitions = [[tuple(x) for x in part] for part in mi.set_partitions(set(encoding.CLASSES) - {'R1', 'R2', 'R3', 'R4'})]\n",
    "    for i, part in enumerate(partitions, 1):\n",
    "        print(f'{i}/{len(partitions)}: {part}')\n",
    "        model, stats = experiment(part)\n",
    "\n",
    "experiment_partitions()\n",
    "# TODO: statistics for each k in each combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'R1'\n",
    "labels = [x[::-1] for x in CLASSES[k]]\n",
    "ax = sn.heatmap(stats.confusion[k], xticklabels=labels, yticklabels=labels, square=True, robust=True, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sn.heatmap(stats.confusion_logprobs[k], xticklabels=labels, yticklabels=labels, square=True, robust=True, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, 'סבסו'))\n",
    "print(predict(model, 'מקדו'))\n",
    "print(predict(model, 'נמזר'))\n",
    "print(predict(model, 'כרדו'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, 'הבריל'))\n",
    "print(predict(model, 'חגוו'))\n",
    "print(predict(model, 'עגו'))\n",
    "print(predict(model, 'צירלל'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, \"השטקרפתי\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(model, \"ישסו\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import encoding\n",
    "import naive_model\n",
    "import utils\n",
    "encoding = importlib.reload(encoding)\n",
    "naive_model = importlib.reload(naive_model)\n",
    "utils = importlib.reload(utils)\n",
    "wandb = importlib.reload(wandb)\n",
    "Stats = utils.Stats\n",
    "NaiveModel = naive_model.NaiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_all_ys(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[('B', 'T')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ravel_multi_index(pre_test_y, [('B', 'T')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination = ('B', 'T')\n",
    "np.ravel_multi_index([pre_test_y[k] for k in combination], encoding.combined_shape(combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonempty_powerset(seq):\n",
    "    return itertools.chain.from_iterable(itertools.combinations(seq, r) for r in range(1, len(seq)+1))\n",
    "\n",
    "def tensor_outer_product(a, b, c):\n",
    "    return torch.einsum('bi,bj,bk->bijk', a, b, c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
