{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from naive_model import NaiveModel\n",
    "import encoding\n",
    "import neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_filename, features, batch_size=64, validation_size=10000):\n",
    "        super().__init__()\n",
    "        WORD_MAXLEN = 11\n",
    "        self.validation_size = validation_size\n",
    "        self.batch_size = batch_size\n",
    "        self.features = features\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        train_data_x, train_data_y = encoding.load_dataset_split(train_filename, split=validation_size)\n",
    "\n",
    "        train = utils.batch_xy((train_x, train_y), config['batch_size'])\n",
    "        test = utils.batch_xy((valx, valy), config['batch_size'])\n",
    "\n",
    "        self.data_train_full = torch.utils.data.TensorDataset(torch.Tensor(train_data_x), *[torch.Tensor(y).to(torch.int64) for y in train_data_y])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # No state assignment here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.data_train, self.data_val = torch.utils.data.random_split(self.data_train_full, [self.validation_size, len(self.data_train_full) - self.validation_size])\n",
    "            self.dims = tuple(self.data_train[0][0].shape)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.dims = tuple(self.data_test[0][0].shape)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 300\n",
    "\n",
    "def load_dataset(corpus_name, artifact_name):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    filename = f'{corpus_name}/{artifact_name}.tsv'  # all_verbs_shuffled\n",
    "\n",
    "    artifact = wandb.Artifact(artifact_name, type='dataset')\n",
    "    artifact.add_file(filename)\n",
    "\n",
    "    (train_x, pre_train_y), (val_x, pre_val_y) = encoding.load_dataset_split(filename, split=val_size)\n",
    "\n",
    "    utils.shuffle_in_unison([train_x, *pre_train_y.values()])\n",
    "    return (train_x, pre_train_y), (val_x, pre_val_y), artifact\n",
    "\n",
    "\n",
    "corpus_name = 'ud'\n",
    "arity = 'combined'\n",
    "gen = 'train'\n",
    "artifact_name = f'nocontext-{gen}'\n",
    "ud_corpus = load_dataset(corpus_name, artifact_name)\n",
    "\n",
    "corpus_name = 'synthetic'\n",
    "arity = 'combined'\n",
    "gen = 'all_pref'\n",
    "artifact_name = f'{gen}_{arity}_shufroot'\n",
    "synthetic_corpus = load_dataset(corpus_name, artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EMBEDDING = 2000\n",
    "\n",
    "class IndependentModel(neural.UdModel):\n",
    "\n",
    "    def __init__(self, combination, units=400, learning_rate=2e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        \n",
    "        self.lstm = neural.SumBiLSTM(units)\n",
    "        \n",
    "        self.tasks = nn.ModuleDict({encoding.class_name(combination): nn.Linear(in_features=units, out_features=encoding.class_size(combination))\n",
    "                                    for combination in combinations})\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (BATCH_SIZE, WORD_MAXLEN)\n",
    "        \n",
    "        x = x.permute([1, 0])\n",
    "        # x: (WORD_MAXLEN, BATCH_SIZE)\n",
    "        \n",
    "        embeds = self.embed(x)\n",
    "        # embeds: (WORD_MAXLEN, BATCH_SIZE, UNITS)\n",
    "        \n",
    "        _, char_hidden = self.lstm(embeds)\n",
    "        # char_hidden: (BATCH_SIZE, UNITS)\n",
    "        \n",
    "        return {name: linear(char_hidden).permute([0, 2, 1])\n",
    "                for name, linear in self.tasks.items()}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [4], gamma=0.5)\n",
    "        return [optimizer], [scheduler]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TEMP_PATH = 'model.pt'\n",
    "\n",
    "#                 best_lr = 8e-4\n",
    "#                 best_loss = 10\n",
    "                \n",
    "#                 torch.save({\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'optimizer': optimizer.state_dict(),\n",
    "#                 }, TEMP_PATH)\n",
    "                \n",
    "#                 for i in range(1):\n",
    "#                     checkpoint = torch.load(TEMP_PATH)\n",
    "#                     model.load_state_dict(checkpoint['state_dict'])\n",
    "#                     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "def fit(model, train, test, *, epochs,  runsize, criterion, optimizer, batch_size, **_):\n",
    "    train_x, train_y = train\n",
    "    valx, valy = test\n",
    "    \n",
    "    assert_reasonable_initial = utils.Once(utils.assert_reasonable_initial)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_stats = utils.Stats(model.tasks.keys())\n",
    "        \n",
    "        nbatches = len(train_x)\n",
    "        for batch, (inputs, labels) in enumerate(zip(train_x, train_y), 1):\n",
    "            model.train()\n",
    "\n",
    "            inputs = to_device(inputs)\n",
    "            labels = to_device(labels)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            losses = {combination: criterion(output.double(), labels[combination])\n",
    "                      for combination, output in outputs.items()}\n",
    "\n",
    "            loss = sum(losses.values())\n",
    "            \n",
    "            assert_reasonable_initial(losses, nn.CrossEntropyLoss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             scheduler.step()\n",
    "            \n",
    "            train_stats.update(loss=loss.item(),\n",
    "                               batch_size=inputs.size(0),\n",
    "                               outputs=outputs,\n",
    "                               labels=labels)\n",
    "\n",
    "            if batch % runsize == 0 or batch == nbatches:\n",
    "                model.eval()\n",
    "\n",
    "                valstats = utils.Stats(model.tasks.keys())\n",
    "                for inputs, labels in zip(valx, valy):\n",
    "                    inputs = to_device(inputs)\n",
    "                    labels = to_device(labels)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                    losses = {combination: criterion(output.double(), labels[combination])\n",
    "                              for combination, output in outputs.items()}\n",
    "\n",
    "                    loss = sum(losses.values())\n",
    "\n",
    "                    valstats.update(loss=loss.item(),\n",
    "                                      batch_size=inputs.size(0),\n",
    "                                      outputs=outputs,\n",
    "                                      labels=labels)\n",
    "                    \n",
    "                utils.log(train_stats, valstats, batch, nbatches, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "env: WANDB_MODE=dryrun\n",
      "{'epochs': 1, 'test_size': 300, 'batch_size': 128, 'units': 350, 'weight_decay': 0.0007, 'dropout': 0.2, 'num_layers': 1, 'lr': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 1, 'test_size': 300, 'batch_size': 128, 'units': 350, 'weight_decay': 0.0007, 'dropout': 0.2, 'num_layers': 1, 'lr': 0.001, 'runsize': 128, 'optimizer': Adam (1: 0.938 train/Accuracy_R2: 0.761 train/Accuracy_R3: 0.985 train/Accuracy_R4: 0.961 train/Accuracy_R1xR2xR3xR4: 0.707 val/Loss: 2.4984 val/Accuracy_B: 0.629 val/Accuracy_T: 0.887 val/Accuracy_V: 0.691 val/Accuracy_G: 0.699 val/Accuracy_P: 0.957 val/Accuracy_R1: 0.996 val/Accuracy_R2: 0.996 val/Accuracy_R3: 1.000 val/Accuracy_R4: 0.938 val/Accuracy_R1xR2xR3xR4: 0.930 \n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0007\n",
      "), 'criterion': CrossEntropyLoss(), 'model': Model(\n",
      "  (embed): Embedding(2000, 350)\n",
      "  (pre_lstm): LSTM(350, 350, bidirectional=True)\n",
      "  (post_lstm): LSTM(350, 350, bidirectional=True)\n",
      "  (B): Linear(in_features=350, out_features=7, bias=True)\n",
      "  (T): Linear(in_features=350, out_features=4, bias=True)\n",
      "  (V): Linear(in_features=350, out_features=5, bias=True)\n",
      "  (G): Linear(in_features=350, out_features=4, bias=True)\n",
      "  (P): Linear(in_features=350, out_features=3, bias=True)\n",
      "  (R1): Linear(in_features=350, out_features=27, bias=True)\n",
      "  (R2): Linear(in_features=350, out_features=27, bias=True)\n",
      "  (R3): Linear(in_features=350, out_features=27, bias=True)\n",
      "  (R4): Linear(in_features=350, out_features=27, bias=True)\n",
      ")}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    90/   90 train/Loss: 1.8064 train/Accuracy_B: 0.924 train/Accuracy_T: 0.941 train/Accuracy_V: 0.899 train/Accuracy_G: 0.924 train/Accuracy_P: 0.973 train/Accuracy_R1: 0.945 train/Accuracy_R2: 0.935 train/Accuracy_R3: 0.991 train/Accuracy_R4: 0.966 train/Accuracy_R1xR2xR3xR4: 0.879 val/Loss: 0.7256 val/Accuracy_B: 0.973 val/Accuracy_T: 0.980 val/Accuracy_V: 0.969 val/Accuracy_G: 0.969 val/Accuracy_P: 0.984 val/Accuracy_R1: 0.973 val/Accuracy_R2: 0.977 val/Accuracy_R3: 1.000 val/Accuracy_R4: 0.984 val/Accuracy_R1xR2xR3xR4: 0.941 \r"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT true\n",
    "\n",
    "def experiment(corpus, config, combinations=encoding.NAMES, names_str=''):\n",
    "    print(config)\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    (train_x, pre_train_y), (valx, pre_valy), artifact = corpus\n",
    "    \n",
    "    train_y = utils.ravel_multi_index(pre_train_y, combinations)\n",
    "    valy = utils.ravel_multi_index(pre_valy, combinations)\n",
    "    \n",
    "    train = utils.batch_xy((train_x, train_y), config['batch_size'])\n",
    "    test = utils.batch_xy((valx, valy), config['batch_size'])\n",
    "    \n",
    "    if corpus is synthetic_corpus:\n",
    "        model = to_device(Model(units=config['units'], combinations=combinations))  # NaiveModel.learn_from_file(filename)\n",
    "    else:\n",
    "        model = torch.load(f\"models/pretrain.pt\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    config.update({\n",
    "        'runsize': 2 * 8192 // config['batch_size'],\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'model': model,\n",
    "    })\n",
    "    \n",
    "#     names_str = '+'.join(encoding.class_name(combination) for combination in combinations if combination not in encoding.NONROOTS)\n",
    "#     if len(combinations) <= 3:\n",
    "#         names_str += '_only'\n",
    "    run = wandb.init(project=\"rootem\",\n",
    "                     group=f'ud',  # f'lr_units_grid_search-{arity}-{wandb.util.generate_id()}',\n",
    "                     name=f\"pretrained-batch_{config['batch_size']}\",  # {model.arch}-{config['units']}-{config['lr']:.0e}-{config['batch_size']} f'{gen}-{arity}-{lr:.0e}',# f'{arity}-batch_{BATCH_SIZE}', # f'all-{arity}-lr_{lr:.0e}-units_{units}',\n",
    "                     tags=[gen, arity, \"ud\", 'shuffle-root', 'shuffle', 'batchval', 'full-root'],\n",
    "                     config=config)\n",
    "    with run:\n",
    "        run.use_artifact(artifact)\n",
    "\n",
    "        wandb.config.update(config, allow_val_change=True)\n",
    "\n",
    "#         if isinstance(model, nn.Module):\n",
    "#             wandb.watch(model)\n",
    "\n",
    "        fit(train=train,\n",
    "            test=test,\n",
    "            **config\n",
    "        )\n",
    "        wandb.save(f\"{model.arch}.h5\")\n",
    "        \n",
    "        if corpus is synthetic_corpus:\n",
    "            torch.save(model, f\"models/pretrain.pt\")\n",
    "        else:\n",
    "            torch.save(model, f\"models/postrain.pt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "%env WANDB_MODE dryrun\n",
    "\n",
    "config = {\n",
    "    'epochs': 1,\n",
    "    'valsize': valsize,\n",
    "    'batch_size': 128,\n",
    "    'units': 350,\n",
    "    'weight_decay': 7e-4,\n",
    "    'dropout': 0.2,\n",
    "    'num_layers': 1,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "model = experiment(synthetic_corpus, config)\n",
    "model = experiment(ud_corpus, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, *verbs):\n",
    "    model.eval()\n",
    "    verbs = encoding.wordlist2numpy(verbs * 128)\n",
    "    verbs = to_device(torch.from_numpy(verbs).to(torch.int64))\n",
    "    outputs = {k: v[0] for k, v in model(verbs).items()}\n",
    "    res = {}\n",
    "    # FIX: assumes no overlaps\n",
    "    for combination, v in outputs.items():\n",
    "        if isinstance(combination, str):\n",
    "            combination = tuple([combination])\n",
    "        shape = encoding.combined_shape(combination)\n",
    "        combined_index = v.argmax().cpu().data.numpy()\n",
    "        indices = np.unravel_index(combined_index, shape)\n",
    "        for k, i in zip(combination, indices):\n",
    "            # assert k not in res, \"Overlapping classes are not handled\"\n",
    "            s = k\n",
    "            if k in res:\n",
    "                s += \"'\"\n",
    "            res[s] = encoding.from_category(k, i)\n",
    "    if all(r in res for r in ['R1', 'R2', 'R3', 'R4']):\n",
    "        res['R'] = ''.join(res[k] for k in ['R1', 'R2', 'R3', 'R4']).replace('.', '')\n",
    "    return '\\t'.join(f'{v:>6}' for k, v in res.items() if k not in ['R1', 'R2', 'R3', 'R4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "השתזף  התפעל\t ציווי\t שלישי\t   זכר\t  יחיד\t   שזפ\n",
      "שמרתי    פעל\t   עבר\t ראשון\t   זכר\t  יחיד\t   שמר\n",
      "ירעדו    פעל\t  עתיד\t שלישי\t   זכר\t  רבים\t   רעד\n",
      "נאכל   נפעל\t  הווה\t ראשון\t   זכר\t  יחיד\t   אכל\n",
      "הרבינו  הפעיל\t   עבר\t ראשון\t   זכר\t  רבים\t   רבי\n",
      "כשהתעצבנתם  התפעל\t   עבר\t   שני\t   זכר\t  רבים\t  עצבנ\n",
      "השגנו  הפעיל\t   עבר\t ראשון\t   זכר\t  רבים\t   שגג\n",
      "תרגלתי   פיעל\t   עבר\t ראשון\t  נקבה\t  יחיד\t  תגגל\n",
      "עופו   פועל\t   עבר\t שלישי\t   זכר\t  רבים\t   עפפ\n",
      "פיהקתם   פיעל\t   עבר\t   שני\t   זכר\t  רבים\t   פהק\n",
      "צפינו    פעל\t   עבר\t ראשון\t  נקבה\t  רבים\t   צפי\n",
      "הצפינו  הפעיל\t   עבר\t ראשון\t   זכר\t  רבים\t   צפי\n",
      "שרנו    פעל\t   עבר\t ראשון\t   זכר\t  רבים\t   שיר\n",
      "להתווכח  התפעל\t  הווה\t     _\t     _\t     _\t   וכח\n",
      "תוכיחי  הפעיל\t  עתיד\t   שני\t  נקבה\t  יחיד\t   יכח\n",
      "קומו    פעל\t   עבר\t שלישי\t   זכר\t  רבים\t   קומ\n",
      "חבל    פעל\t   עבר\t שלישי\t   זכר\t  יחיד\t   חבל\n"
     ]
    }
   ],
   "source": [
    "s = 'השתזף שמרתי ירעדו נאכל הרבינו כשהתעצבנתם השגנו תרגלתי עופו פיהקתם צפינו הצפינו שרנו להתווכח תוכיחי קומו'\n",
    "\n",
    "model = torch.load(f\"models/pretrain.pt\")\n",
    "for k in s.split():\n",
    "    print(k, predict(model, k))\n",
    "print(\"חבל\", predict(model, \"חבל\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
