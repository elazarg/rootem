{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import verbs\n",
    "import neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_filename, features, batch_size=64, validation_size=10000):\n",
    "        super().__init__()\n",
    "        WORD_MAXLEN = 11\n",
    "        self.validation_size = validation_size\n",
    "        self.batch_size = batch_size\n",
    "        self.features = features\n",
    "        \n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        train_data_x, train_data_y = verbs.load_dataset(train_filename, word_maxlen=WORD_MAXLEN)\n",
    "\n",
    "        self.data_train_full = torch.utils.data.TensorDataset(torch.Tensor(train_data_x), *[torch.Tensor(y).to(torch.int64) for y in train_data_y])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # No state assignment here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.data_train, self.data_val = torch.utils.data.random_split(self.data_train_full, [self.validation_size, len(self.data_train_full) - self.validation_size])\n",
    "            self.dims = tuple(self.data_train[0][0].shape)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.dims = tuple(self.data_test[0][0].shape)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EMBEDDING = 2000\n",
    "\n",
    "class IndependentModel(neural.UdModel):\n",
    "\n",
    "    def __init__(self, label_names, units=400, learning_rate=2e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        \n",
    "        self.lstm = neural.SumBiLSTM(units)\n",
    "        \n",
    "        self.tasks = nn.ModuleDict({label_name: nn.Linear(in_features=units, out_features=verbs.Verb.class_size(label_name))\n",
    "                                    for label_name in label_names})\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (BATCH_SIZE, WORD_MAXLEN)\n",
    "        \n",
    "        x = x.permute([1, 0])\n",
    "        # x: (WORD_MAXLEN, BATCH_SIZE)\n",
    "        \n",
    "        embeds = self.embed(x)\n",
    "        # embeds: (WORD_MAXLEN, BATCH_SIZE, UNITS)\n",
    "        \n",
    "        _, char_hidden = self.lstm(embeds)\n",
    "        # char_hidden: (BATCH_SIZE, UNITS)\n",
    "        \n",
    "        return {name: linear(char_hidden).permute([0, 2, 1])\n",
    "                for name, linear in self.tasks.items()}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 300\n",
    "\n",
    "def load_dataset(corpus_name, artifact_name):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    filename = f'{corpus_name}/{artifact_name}.tsv'  # all_verbs_shuffled\n",
    "\n",
    "    artifact = wandb.Artifact(artifact_name, type='dataset')\n",
    "    artifact.add_file(filename)\n",
    "\n",
    "    (train_x, pre_train_y), (val_x, pre_val_y) = encoding.load_dataset_split(filename, split=val_size)\n",
    "\n",
    "    utils.shuffle_in_unison([train_x, *pre_train_y.values()])\n",
    "    return (train_x, pre_train_y), (val_x, pre_val_y), artifact\n",
    "\n",
    "\n",
    "corpus_name = 'ud'\n",
    "arity = 'combined'\n",
    "gen = 'train'\n",
    "artifact_name = f'nocontext-{gen}'\n",
    "ud_corpus = load_dataset(corpus_name, artifact_name)\n",
    "\n",
    "corpus_name = 'synthetic'\n",
    "arity = 'combined'\n",
    "gen = 'all_pref'\n",
    "artifact_name = f'{gen}_{arity}_shufroot'\n",
    "synthetic_corpus = load_dataset(corpus_name, artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TEMP_PATH = 'model.pt'\n",
    "\n",
    "#                 best_lr = 8e-4\n",
    "#                 best_loss = 10\n",
    "                \n",
    "#                 torch.save({\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'optimizer': optimizer.state_dict(),\n",
    "#                 }, TEMP_PATH)\n",
    "                \n",
    "#                 for i in range(1):\n",
    "#                     checkpoint = torch.load(TEMP_PATH)\n",
    "#                     model.load_state_dict(checkpoint['state_dict'])\n",
    "#                     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "def fit(model, train, test, *, epochs,  runsize, criterion, optimizer, batch_size, **_):\n",
    "    train_x, train_y = train\n",
    "    valx, valy = test\n",
    "    \n",
    "    assert_reasonable_initial = utils.Once(utils.assert_reasonable_initial)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_stats = utils.Stats(model.tasks.keys())\n",
    "        \n",
    "        nbatches = len(train_x)\n",
    "        for batch, (inputs, labels) in enumerate(zip(train_x, train_y), 1):\n",
    "            model.train()\n",
    "\n",
    "            inputs = to_device(inputs)\n",
    "            labels = to_device(labels)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            losses = {combination: criterion(output.double(), labels[combination])\n",
    "                      for combination, output in outputs.items()}\n",
    "\n",
    "            loss = sum(losses.values())\n",
    "            \n",
    "            assert_reasonable_initial(losses, nn.CrossEntropyLoss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             scheduler.step()\n",
    "            \n",
    "            train_stats.update(loss=loss.item(),\n",
    "                               batch_size=inputs.size(0),\n",
    "                               outputs=outputs,\n",
    "                               labels=labels)\n",
    "\n",
    "            if batch % runsize == 0 or batch == nbatches:\n",
    "                model.eval()\n",
    "\n",
    "                valstats = utils.Stats(model.tasks.keys())\n",
    "                for inputs, labels in zip(valx, valy):\n",
    "                    inputs = to_device(inputs)\n",
    "                    labels = to_device(labels)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                    losses = {combination: criterion(output.double(), labels[combination])\n",
    "                              for combination, output in outputs.items()}\n",
    "\n",
    "                    loss = sum(losses.values())\n",
    "\n",
    "                    valstats.update(loss=loss.item(),\n",
    "                                      batch_size=inputs.size(0),\n",
    "                                      outputs=outputs,\n",
    "                                      labels=labels)\n",
    "                    \n",
    "                utils.log(train_stats, valstats, batch, nbatches, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env WANDB_SILENT true\n",
    "\n",
    "def experiment(corpus, config, combinations=encoding.NAMES, names_str=''):\n",
    "    print(config)\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    (train_x, pre_train_y), (valx, pre_valy), artifact = corpus\n",
    "    \n",
    "    train_y = utils.ravel_multi_index(pre_train_y, combinations)\n",
    "    valy = utils.ravel_multi_index(pre_valy, combinations)\n",
    "    \n",
    "    train = utils.batch_xy((train_x, train_y), config['batch_size'])\n",
    "    test = utils.batch_xy((valx, valy), config['batch_size'])\n",
    "    \n",
    "    if corpus is synthetic_corpus:\n",
    "        model = to_device(Model(units=config['units'], combinations=combinations))  # NaiveModel.learn_from_file(filename)\n",
    "    else:\n",
    "        model = torch.load(f\"models/pretrain.pt\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    config.update({\n",
    "        'runsize': 2 * 8192 // config['batch_size'],\n",
    "        'optimizer': optimizer,\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'model': model,\n",
    "    })\n",
    "    \n",
    "#     names_str = '+'.join(encoding.class_name(combination) for combination in combinations if combination not in encoding.NONROOTS)\n",
    "#     if len(combinations) <= 3:\n",
    "#         names_str += '_only'\n",
    "    run = wandb.init(project=\"rootem\",\n",
    "                     group=f'ud',  # f'lr_units_grid_search-{arity}-{wandb.util.generate_id()}',\n",
    "                     name=f\"pretrained-batch_{config['batch_size']}\",  # {model.arch}-{config['units']}-{config['lr']:.0e}-{config['batch_size']} f'{gen}-{arity}-{lr:.0e}',# f'{arity}-batch_{BATCH_SIZE}', # f'all-{arity}-lr_{lr:.0e}-units_{units}',\n",
    "                     tags=[gen, arity, \"ud\", 'shuffle-root', 'shuffle', 'batchval', 'full-root'],\n",
    "                     config=config)\n",
    "    with run:\n",
    "        run.use_artifact(artifact)\n",
    "\n",
    "        wandb.config.update(config, allow_val_change=True)\n",
    "\n",
    "#         if isinstance(model, nn.Module):\n",
    "#             wandb.watch(model)\n",
    "\n",
    "        fit(train=train,\n",
    "            test=test,\n",
    "            **config\n",
    "        )\n",
    "        wandb.save(f\"{model.arch}.h5\")\n",
    "        \n",
    "        if corpus is synthetic_corpus:\n",
    "            torch.save(model, f\"models/pretrain.pt\")\n",
    "        else:\n",
    "            torch.save(model, f\"models/postrain.pt\")\n",
    "\n",
    "    return model\n",
    "\n",
    "%env WANDB_MODE dryrun\n",
    "\n",
    "config = {\n",
    "    'epochs': 1,\n",
    "    'valsize': valsize,\n",
    "    'batch_size': 128,\n",
    "    'units': 350,\n",
    "    'weight_decay': 7e-4,\n",
    "    'dropout': 0.2,\n",
    "    'num_layers': 1,\n",
    "    'lr': 1e-3,\n",
    "}\n",
    "model = experiment(synthetic_corpus, config)\n",
    "model = experiment(ud_corpus, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, *verbs):\n",
    "    model.eval()\n",
    "    verbs = encoding.wordlist2numpy(verbs * 128)\n",
    "    verbs = to_device(torch.from_numpy(verbs).to(torch.int64))\n",
    "    outputs = {k: v[0] for k, v in model(verbs).items()}\n",
    "    res = {}\n",
    "    # FIX: assumes no overlaps\n",
    "    for combination, v in outputs.items():\n",
    "        if isinstance(combination, str):\n",
    "            combination = tuple([combination])\n",
    "        shape = encoding.combined_shape(combination)\n",
    "        combined_index = v.argmax().cpu().data.numpy()\n",
    "        indices = np.unravel_index(combined_index, shape)\n",
    "        for k, i in zip(combination, indices):\n",
    "            # assert k not in res, \"Overlapping classes are not handled\"\n",
    "            s = k\n",
    "            if k in res:\n",
    "                s += \"'\"\n",
    "            res[s] = encoding.from_category(k, i)\n",
    "    if all(r in res for r in ['R1', 'R2', 'R3', 'R4']):\n",
    "        res['R'] = ''.join(res[k] for k in ['R1', 'R2', 'R3', 'R4']).replace('.', '')\n",
    "    return '\\t'.join(f'{v:>6}' for k, v in res.items() if k not in ['R1', 'R2', 'R3', 'R4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = 'השתזף שמרתי ירעדו נאכל הרבינו כשהתעצבנתם השגנו תרגלתי עופו פיהקתם צפינו הצפינו שרנו להתווכח תוכיחי קומו'\n",
    "\n",
    "model = torch.load(f\"models/pretrain.pt\")\n",
    "for k in s.split():\n",
    "    print(k, predict(model, k))\n",
    "print(\"חבל\", predict(model, \"חבל\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
