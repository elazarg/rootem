{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elaza\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_lightning\\utilities\\distributed.py:25: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import utils\n",
    "import encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 300\n",
    "SENTENCE_MAXLEN = 30\n",
    "WORD_MAXLEN = 11\n",
    "\n",
    "def load_dataset(kind):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    filename = f'../Hebrew_UD/he_htb-ud-{kind}.conllu'\n",
    "\n",
    "#     artifact = wandb.Artifact(artifact_name, type='dataset')\n",
    "#     artifact.add_file(filename)\n",
    "\n",
    "    (train_x, train_y), (test_x, test_y) = encoding.load_sentences_split(filename, test_size, SENTENCE_MAXLEN, WORD_MAXLEN)\n",
    "\n",
    "    # utils.shuffle_in_unison([train_x, *pre_train_y.values()])\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "kind = 'dev'\n",
    "(train_x, train_y), (test_x, test_y) = load_dataset(kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMBEDDING = 2000\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.lstm = nn.LSTM(input_size=units, hidden_size=units, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., UNITS)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out: (..., UNITS * 2)\n",
    "        # hidden: (2, ..., UNITS)\n",
    "        # cell: (2, ..., UNITS)\n",
    "        \n",
    "        hidden = hidden[0] + hidden[1]\n",
    "        # hidden: (..., UNITS)\n",
    "        \n",
    "        left, right = torch.chunk(lstm_out, 2, dim=-1)\n",
    "        # left: (..., UNITS)\n",
    "        # right: (..., UNITS)\n",
    "        \n",
    "        lstm_out = torch.squeeze(left + right)\n",
    "        # lstm_out: (..., UNITS)\n",
    "        \n",
    "        return lstm_out, hidden\n",
    "    \n",
    "    \n",
    "class IndependentModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        \n",
    "        self.char_lstm1 = SimpleLSTM(units)\n",
    "        self.char_lstm2 = SimpleLSTM(units)\n",
    "        \n",
    "        self.word_lstm1 = SimpleLSTM(units)\n",
    "        self.word_lstm2 = SimpleLSTM(units)\n",
    "\n",
    "        self.pos = nn.Linear(in_features=units, out_features=len(encoding.Classes.xpos))\n",
    "        self.binyan = nn.Linear(in_features=units, out_features=len(encoding.Classes.HebBinyan))\n",
    "        self.r1 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r2 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r3 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r4 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.int64)\n",
    "        # x: (BATCH_SIZE, SENT_MAXLEN, WORD_MAXLEN)\n",
    "        \n",
    "        # Step 0: character embedding \n",
    "        \n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        # x: (SENT_MAXLEN * BATCH_SIZE, WORD_MAXLEN)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        # embeds: (WORD_MAXLEN, SENT_MAXLEN * BATCH_SIZE, UNITS)\n",
    "        \n",
    "        embeds = embeds.permute([1, 0, 2])\n",
    "        # x: (SENT_MAXLEN * BATCH_SIZE, WORD_MAXLEN, UNITS)\n",
    "\n",
    "        \n",
    "        # STEP 1: character-level lstm -> word embedding\n",
    "        \n",
    "        char_lstm_out, _ = self.char_lstm1(embeds)\n",
    "        # char_lstm_out: (WORD_MAXLEN, SENT_MAXLEN * BATCH_SIZE, UNITS)\n",
    "\n",
    "        _, char_hidden = self.char_lstm2(char_lstm_out)\n",
    "        # char_hidden: (SENT_MAXLEN * BATCH_SIZE, UNITS)\n",
    "        \n",
    "        char_hidden = char_hidden.reshape(SENTENCE_MAXLEN, -1, self.units)\n",
    "        # char_hidden: (SENT_MAXLEN, BATCH_SIZE, UNITS)\n",
    " \n",
    "\n",
    "        # STEP 2: sequence tagging using word-level lstm\n",
    "    \n",
    "        word_lstm_out, _ = self.word_lstm1(char_hidden)\n",
    "        # word_lstm_out: (SENT_MAXLEN, BATCH_SIZE, UNITS)\n",
    "\n",
    "        word_lstm_out += self.char_lstm2(word_lstm_out)[0]\n",
    "        # word_lstm_out: (SENT_MAXLEN, BATCH_SIZE, UNITS)\n",
    "        \n",
    "        word_lstm_out = word_lstm_out.permute([1, 0, 2])\n",
    "        # word_lstm_out: (BATCH_SIZE, SENT_MAXLEN, UNITS)\n",
    "        \n",
    "        return [linear(word_lstm_out) for linear in [self.pos, self.binyan, self.r1, self.r2, self.r3, self.r4]]\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return { 'loss': loss }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.003)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        return {'test_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'avg_test_loss': avg_loss }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IndependentModel(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(torch.tensor(train_x[:64, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([64, 30, 16]),\n",
       " torch.Size([64, 30, 8]),\n",
       " torch.Size([64, 30, 27]),\n",
       " torch.Size([64, 30, 27]),\n",
       " torch.Size([64, 30, 27]),\n",
       " torch.Size([64, 30, 27])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 30)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0, :64].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IndependentModel()\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
