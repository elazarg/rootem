{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import utils\n",
    "import encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, conllu_filename, batch_size=64):\n",
    "        super().__init__()\n",
    "        SENTENCE_MAXLEN = 30\n",
    "        WORD_MAXLEN = 11\n",
    "        self.batch_size = batch_size\n",
    "        data_x, data_y = encoding.load_sentences(conllu_filename, SENTENCE_MAXLEN, WORD_MAXLEN)\n",
    "        self.data = torch.utils.data.TensorDataset(torch.Tensor(data_x), *[torch.Tensor(y).to(torch.int64) for y in data_y])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # No state assignment here\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        val_size = 300\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.data_train, self.data_val = torch.utils.data.random_split(self.data, [val_size, len(self.data) - val_size])\n",
    "            self.dims = tuple(self.data_train[0][0].shape)\n",
    "\n",
    "        if stage == 'test': # or stage is None:\n",
    "            assert False\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.data_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        assert False\n",
    "        return torch.utils.data.DataLoader(self.data_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMBEDDING = 2000\n",
    "\n",
    "class SumBiLSTM(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.lstm = nn.LSTM(input_size=units, hidden_size=units, num_layers=1, batch_first=False, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., UNITS)\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out: (..., UNITS * 2)\n",
    "        # hidden: (2, ..., UNITS)\n",
    "        # cell: (2, ..., UNITS)\n",
    "        \n",
    "        hidden = hidden[0] + hidden[1]\n",
    "        # hidden: (..., UNITS)\n",
    "        \n",
    "        left, right = torch.chunk(lstm_out, 2, dim=-1)\n",
    "        # left: (..., UNITS)\n",
    "        # right: (..., UNITS)\n",
    "        \n",
    "        lstm_out = torch.squeeze(left + right)\n",
    "        # lstm_out: (..., UNITS)\n",
    "        \n",
    "        return lstm_out, hidden\n",
    "    \n",
    "    \n",
    "class IndependentModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=NUM_EMBEDDING, embedding_dim=units)\n",
    "        \n",
    "        self.char_lstm1 = SumBiLSTM(units)\n",
    "        self.char_lstm2 = SumBiLSTM(units)\n",
    "        \n",
    "        self.word_lstm1 = SumBiLSTM(units)\n",
    "        self.word_lstm2 = SumBiLSTM(units)\n",
    "\n",
    "        self.pos = nn.Linear(in_features=units, out_features=len(encoding.Classes.xpos))\n",
    "        self.binyan = nn.Linear(in_features=units, out_features=len(encoding.Classes.HebBinyan))\n",
    "        self.r1 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r2 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r3 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "        self.r4 = nn.Linear(in_features=units, out_features=len(encoding.RADICALS))\n",
    "\n",
    "    def forward(self, x):\n",
    "        SENT_MAXLEN = x.shape[1]\n",
    "        \n",
    "        x = x.to(torch.int64)\n",
    "        # x: (BATCH_SIZE, SENT_MAXLEN, WORD_MAXLEN)\n",
    "        \n",
    "        # Step 0: character embedding \n",
    "        \n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        # x: (BATCH_SIZE * SENT_MAXLEN, WORD_MAXLEN)\n",
    "\n",
    "        embeds = self.embed(x)\n",
    "        # embeds: (BATCH_SIZE * SENT_MAXLEN, WORD_MAXLEN, UNITS)\n",
    "        \n",
    "        embeds = embeds.permute([1, 0, 2])\n",
    "        # x: (WORD_MAXLEN, BATCH_SIZE * SENT_MAXLEN, UNITS)\n",
    "\n",
    "        \n",
    "        # STEP 1: character-level lstm -> word embedding\n",
    "        \n",
    "        _, char_hidden = self.char_lstm2(embeds)\n",
    "        # char_hidden: (BATCH_SIZE * SENT_MAXLEN, UNITS)\n",
    "        \n",
    "        char_hidden = char_hidden.reshape(-1, SENT_MAXLEN, self.units)\n",
    "        # char_hidden: (BATCH_SIZE, SENT_MAXLEN, UNITS)\n",
    " \n",
    "\n",
    "        # STEP 2: sequence tagging using word-level lstm\n",
    "    \n",
    "        word_lstm_out, _ = self.word_lstm1(char_hidden)\n",
    "        # word_lstm_out: (BATCH_SIZE, SENT_MAXLEN, UNITS)\n",
    "\n",
    "        return [linear(word_lstm_out).permute([0, 2, 1])\n",
    "                for linear in [self.pos, self.binyan, self.r1, self.r2, self.r3, self.r4]]\n",
    "\n",
    "    def compute_metrics(self, batch):\n",
    "        x, *ys = batch\n",
    "        ys_hat = self(x)  #  * (y != 0).to(torch.float64)\n",
    "        loss = sum(F.cross_entropy(y_hat, y, ignore_index=0) for y_hat, y in zip(ys_hat, ys))\n",
    "        accuracy = {name: pl.metrics.functional.accuracy(y_hat, y)\n",
    "                    for y_hat, y, name in zip(ys_hat, ys, encoding.names())}\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        loss, accuracy = self.compute_metrics(batch)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "        for k, v in accuracy.items():\n",
    "            result.log(f't_{k}_acc', v, prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.compute_metrics(batch)\n",
    "        result = pl.EvalResult()\n",
    "        result.log('val_loss', loss, prog_bar=True)\n",
    "        for k, v in accuracy.items():\n",
    "            result.log(f'val_{k}_acc', v, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        return self(sentence).argmax(1)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, *ys = batch\n",
    "        ys_hat = self(x)\n",
    "        return {'test_loss': F.cross_entropy(ys_hat, ys)}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        return {'avg_test_loss': avg_loss }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConlluDataModule(f'../Hebrew_UD/he_htb-ud-dev.conllu')\n",
    "dataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name       | Type      | Params\n",
      "------------------------------------------\n",
      "0  | embed      | Embedding | 400 K \n",
      "1  | char_lstm1 | SumBiLSTM | 643 K \n",
      "2  | char_lstm2 | SumBiLSTM | 643 K \n",
      "3  | word_lstm1 | SumBiLSTM | 643 K \n",
      "4  | word_lstm2 | SumBiLSTM | 643 K \n",
      "5  | pos        | Linear    | 3 K   \n",
      "6  | binyan     | Linear    | 1 K   \n",
      "7  | r1         | Linear    | 5 K   \n",
      "8  | r2         | Linear    | 5 K   \n",
      "9  | r3         | Linear    | 5 K   \n",
      "10 | r4         | Linear    | 5 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2d832745da4b238dfeecf95e76facd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IndependentModel(200)\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torch.Tensor([encoding.wordlist2numpy(\"אני הולך לאכול כי באתי לרצות ללכת\".split() + [\"\"]*22, word_maxlen=10)] * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, binyan, *radicals = [x.argmax(1)[0] for x in model(text)]\n",
    "pos = [encoding.Classes.xpos[k] for k in pos]\n",
    "binyan = [encoding.BINYAN[k] for k in binyan]\n",
    "r1, r2, r3, r4 = [[encoding.RADICALS[k] for k in r] for r in radicals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ש', 'ע', 'א', 'ש', 'א', 'פ', 'ח', 'ש', 'ש', 'ש']\n"
     ]
    }
   ],
   "source": [
    "print(r1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
